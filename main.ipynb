{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import gym\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick an Environment\n",
    "CartPole, the MNIST of RL, discrete action, parameterized state\n",
    "\n",
    "Breakout, discrete action, 0 fire, 1 stay, 2 right, 3 left\n",
    "    Notice that breakout-ram is hard because the state is 128 bytes from the ram\n",
    "    and the bytes do not have an intuitive meaning\n",
    "    also notice that 50M samples is typical for DQNs with visual input (refer to rainbow)\n",
    "\n",
    "MountainCar, discrete or continous action, parameterized state\n",
    "Gets reward for climbing up a hill that costs energy,\n",
    "painful exploration is essential\n",
    "\n",
    "Walker, peanlty -100 for falling. \n",
    "The initial greedy strategy may make the agent stand unmoved and prevent falling\n",
    "As a result, intial test reward =0 while initial train reward=100\n",
    "\n",
    "For environments with a large penalty, we should use a large batch when updating Q, in order to compensate the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import FrameStack\n",
    "\n",
    "class BreakoutWrapper(gym.ObservationWrapper):\n",
    "    \"\"\" \n",
    "    takes (210, 160, 3) to (40, 40)\n",
    "    stops training when one life is lost\n",
    "    converts to grey scale float\n",
    "    cuts the margins\n",
    "    \n",
    "    fires the ball by pressing action 1\n",
    "    \n",
    "    wrapped by framestack (not wrapping FrameStack) to utilize lazy frame for memory saving\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.observation_space = gym.spaces.Box(0, 1, (170, 160))\n",
    "        self.pooling = torch.nn.AvgPool2d(kernel_size=(4,4), stride=(4, 4))\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives:\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return self.observation(obs), reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        obs, _, _, _ = self.step(1)\n",
    "        return obs\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = np.array(observation).astype(np.float32) / 255.0\n",
    "        observation = observation[30:-17] \n",
    "        observation = np.mean(observation, axis=2) # greyscale\n",
    "        tmp = torch.as_tensor(observation).unsqueeze(0)\n",
    "        observation = np.array(self.pooling(tmp).squeeze(0))\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "env_name = 'Breakout-v0'\n",
    "env_fn = lambda: FrameStack(BreakoutWrapper(gym.make(env_name)), 4)\n",
    "\n",
    "env = env_fn()\n",
    "result  = np.array(env.reset())\n",
    "result = np.array(result).transpose(1, 2, 0) # 0 is white\n",
    "#plt.imshow(result[:, :, -1], cmap='Greys') \n",
    "plt.imshow(1-result[:, :, 1:4]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartpoleWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "\n",
    "    def observation(self, x):\n",
    "        x = np.array(x, dtype=np.float32)\n",
    "        return x\n",
    "    \n",
    "env_name = 'CartPole-v1'\n",
    "env_fn = lambda: CartpoleWrapper(gym.make(env_name))\n",
    "\n",
    "env = env_fn()\n",
    "result  = np.array(env.reset())\n",
    "print(result, result.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick an Agent\n",
    "    QLearning\n",
    "    SAC-discrete, tested on CartPole, run on Breakout\n",
    "    MBPO\n",
    "\n",
    "In general, when an algo does not work, try large batch low lr with few updates\n",
    "\n",
    "It is okay if the loss of Q increases significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Config\n",
    "from models import MLP\n",
    "from agents import MBPO\n",
    "\"\"\"\n",
    "    the hyperparameters are the same as MBPO\n",
    "\"\"\"\n",
    "algo_args = Config()\n",
    "\n",
    "algo_args.n_warmup=0#int(5e3)\n",
    "\"\"\"\n",
    " rainbow said 2e5 samples or 5e4 updates is typical for Qlearning\n",
    " bs256lr3e-4, it takes 2e4updates\n",
    " for the model on CartPole to learn done...\n",
    "\n",
    " Only 3e5 samples are needed for parameterized input continous motion control\n",
    "\"\"\"\n",
    "algo_args.replay_size=int(1e6)\n",
    "algo_args.max_ep_len=500\n",
    "algo_args.test_interval = int(1e4)\n",
    "algo_args.seed=0\n",
    "algo_args.batch_size=256 # the same as MBPO\n",
    "algo_args.save_interval=600 # in seconds\n",
    "algo_args.log_interval=int(2e3/200)\n",
    "algo_args.n_step=int(1e8)\n",
    "\n",
    "p_args=Config()\n",
    "p_args.network = MLP\n",
    "p_args.activation=torch.nn.ReLU\n",
    "p_args.lr=3e-4\n",
    "p_args.sizes = [4, 16, 32, 3] \n",
    "p_args.update_interval=1/10\n",
    "\"\"\"\n",
    " bs=32 interval=4 from rainbow Q\n",
    " MBPO retrains fram scratch periodically\n",
    " in principle this can be arbitrarily frequent\n",
    "\"\"\"\n",
    "p_args.n_p=7 # ensemble\n",
    "p_args.refresh_interval=1#int(1e3) # refreshes the model buffer\n",
    "# ideally rollouts should be used only once\n",
    "p_args.branch=400\n",
    "p_args.roll_length=1 # length > 1 not implemented yet\n",
    "\n",
    "q_args=Config()\n",
    "q_args.network = MLP\n",
    "q_args.activation=torch.nn.ReLU\n",
    "q_args.lr=3e-4\n",
    "q_args.sizes = [4, 16, 32, 3] # 2 actions, dueling q learning\n",
    "q_args.update_interval=1/20\n",
    "# MBPO used 1/40 for continous control tasks\n",
    "# 1/20 for invert pendulum\n",
    "\n",
    "pi_args=Config()\n",
    "pi_args.network = MLP\n",
    "pi_args.activation=torch.nn.ReLU\n",
    "pi_args.lr=3e-4\n",
    "pi_args.sizes = [4, 16, 32, 2] \n",
    "pi_args.update_interval=1/20\n",
    "\n",
    "agent_args=Config()\n",
    "agent_args.agent=MBPO\n",
    "agent_args.gamma=0.99\n",
    "agent_args.alpha=0.2 \n",
    "agent_args.target_sync_rate=5e-3\n",
    "# called tau in MBPO\n",
    "# sync rate per update = update interval/target sync interval\n",
    "\n",
    "args = Config()\n",
    "args.env_name=env_name\n",
    "args.name=f\"{args.env_name}_{agent_args.agent}\"\n",
    "device = 0\n",
    "\n",
    "q_args.env_fn = env_fn\n",
    "agent_args.env_fn = env_fn\n",
    "algo_args.env_fn = env_fn\n",
    "\n",
    "agent_args.p_args = p_args\n",
    "agent_args.q_args = q_args\n",
    "agent_args.pi_args = pi_args\n",
    "algo_args.agent_args = agent_args\n",
    "args.algo_args = algo_args # do not call toDict() before config is set\n",
    "\n",
    "print(f\"rollout reuse:{(p_args.refresh_interval/q_args.update_interval*algo_args.batch_size)/algo_args.replay_size}\")\n",
    "# each generated data will be used so many times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithm import RL\n",
    "from utils import Logger\n",
    "\n",
    "RL(logger = Logger(args), device=device, **algo_args._toDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    tmp = torch.tensor(state).float()\n",
    "    action = model.act(tmp)\n",
    "   # action = env.action_space.sample()\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    tmp = torch.as_tensor(state,  dtype=torch.float).to(device)\n",
    "    action = model.act(tmp, deterministic=False)\n",
    "   # action = env.action_space.sample()\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = plt.imshow(env.reset()[-1]) # only call this once\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    tmp = input()\n",
    "    if len(tmp) == 0:\n",
    "        tmp = \"0\"\n",
    "    action = int(tmp)\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    img.set_data(state[-1]) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "\n",
    "    print(f\"this: {reward}, total: {total}\")\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, **kwargs):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(**{'x':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x=torch.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7376],\n",
       "        [0.7376]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([x, x], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
